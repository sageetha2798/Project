{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2WUhOxgSMHc"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Advanced Time Series Forecasting with Attention-Based LSTM\n",
        "# End-to-end script for Cultus Job Readiness Project\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Reproducibility\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Generate multivariate time series data\n",
        "# -----------------------------\n",
        "def generate_multivariate_series(n_samples=3000, n_features=5):\n",
        "    \"\"\"\n",
        "    Create a synthetic multivariate time series using make_regression.\n",
        "    We then add some non-linearities and noise to make the task harder.\n",
        "    \"\"\"\n",
        "    X, y = make_regression(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "        n_informative=4,\n",
        "        noise=0.5,\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Rescale target and add some non-linearity (simulate complex dynamics)\n",
        "    y = y / 50.0\n",
        "    y = y + 0.2 * np.sin(np.linspace(0, 20, n_samples))  # trend/seasonality\n",
        "\n",
        "    data = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(n_features)])\n",
        "    data[\"target\"] = y\n",
        "\n",
        "    # Introduce some artificial missing values\n",
        "    for col in data.columns:\n",
        "        idx = np.random.choice(n_samples, size=int(0.01 * n_samples), replace=False)\n",
        "        data.loc[idx, col] = np.nan\n",
        "\n",
        "    # Create a time index (simulating daily data)\n",
        "    data.index = pd.date_range(start=\"2015-01-01\", periods=n_samples, freq=\"D\")\n",
        "    return data\n",
        "\n",
        "data = generate_multivariate_series()\n",
        "print(\"Raw data shape:\", data.shape)\n",
        "print(data.head())\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Preprocessing\n",
        "#    - Handle missing values\n",
        "#    - Scale features\n",
        "#    - Create supervised sequences (X windows, y)\n",
        "# -----------------------------\n",
        "def preprocess_data(df, target_col=\"target\"):\n",
        "    # 1) Missing values – forward fill then back fill as backup\n",
        "    df = df.copy()\n",
        "    df = df.ffill().bfill()\n",
        "\n",
        "    # 2) Separate features and target\n",
        "    feature_cols = [c for c in df.columns if c != target_col]\n",
        "    X_values = df[feature_cols].values\n",
        "    y_values = df[target_col].values\n",
        "\n",
        "    # 3) Scale features (but NOT target – metrics should be on original scale)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_values)\n",
        "\n",
        "    processed = pd.DataFrame(\n",
        "        np.column_stack([X_scaled, y_values]),\n",
        "        index=df.index,\n",
        "        columns=feature_cols + [target_col]\n",
        "    )\n",
        "\n",
        "    return processed, scaler, feature_cols, target_col\n",
        "\n",
        "processed_data, scaler, feature_cols, target_col = preprocess_data(data)\n",
        "print(\"Processed data shape:\", processed_data.shape)\n",
        "\n",
        "def create_sequences(values, target_index, seq_len):\n",
        "    \"\"\"\n",
        "    values : np.array of shape (T, D)\n",
        "    target_index : index of the target column within values\n",
        "    seq_len : length of input window\n",
        "\n",
        "    Returns:\n",
        "        X_seq: (num_samples, seq_len, D)\n",
        "        y_seq: (num_samples,)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(values) - seq_len):\n",
        "        X.append(values[i:i+seq_len, :])\n",
        "        # Predict the next-step target after the window\n",
        "        y.append(values[i+seq_len, target_index])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Train/Val/Test split (time-based)\n",
        "# -----------------------------\n",
        "def time_series_train_val_test_split(values, train_ratio=0.6, val_ratio=0.2):\n",
        "    T = len(values)\n",
        "    train_end = int(T * train_ratio)\n",
        "    val_end = int(T * (train_ratio + val_ratio))\n",
        "\n",
        "    train = values[:train_end]\n",
        "    val = values[train_end:val_end]\n",
        "    test = values[val_end:]\n",
        "\n",
        "    return train, val, test\n",
        "\n",
        "all_values = processed_data.values  # [features + target]\n",
        "target_index = processed_data.columns.get_loc(target_col)\n",
        "\n",
        "train_values, val_values, test_values = time_series_train_val_test_split(all_values)\n",
        "\n",
        "print(\"Train/Val/Test sizes:\", train_values.shape, val_values.shape, test_values.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Custom Attention Layer\n",
        "# -----------------------------\n",
        "class AttentionLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    Simple Bahdanau-style attention over LSTM outputs.\n",
        "    Input: LSTM outputs with shape (batch, time, features)\n",
        "    Output: context vector (batch, features) and attention weights (batch, time, 1)\n",
        "    \"\"\"\n",
        "    def __init__(self, units):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.W1 = layers.Dense(units)\n",
        "        self.W2 = layers.Dense(units)\n",
        "        self.V = layers.Dense(1)\n",
        "\n",
        "    def call(self, hidden_states, last_hidden_state):\n",
        "        # hidden_states: (batch, time, features)\n",
        "        # last_hidden_state: (batch, features)\n",
        "        # Expand last_hidden_state to (batch, time, features)\n",
        "        last_hidden_state_expanded = tf.expand_dims(last_hidden_state, 1)\n",
        "\n",
        "        score = tf.nn.tanh(self.W1(hidden_states) + self.W2(last_hidden_state_expanded))\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)  # (batch, time, 1)\n",
        "\n",
        "        context_vector = attention_weights * hidden_states\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)  # (batch, features)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Model builders\n",
        "# -----------------------------\n",
        "def build_baseline_lstm(input_shape, lstm_units=64, learning_rate=1e-3):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LSTM(lstm_units)(inputs)\n",
        "    x = layers.Dense(32, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=\"mse\"\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def build_attention_lstm(input_shape, lstm_units=64, attention_units=32, learning_rate=1e-3):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    lstm_out, state_h, state_c = layers.LSTM(\n",
        "        lstm_units, return_sequences=True, return_state=True\n",
        "    )(inputs)\n",
        "\n",
        "    attention = AttentionLayer(attention_units)\n",
        "    context_vector, attention_weights = attention(lstm_out, state_h)\n",
        "\n",
        "    x = layers.Concatenate()([context_vector, state_h])\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=\"mse\"\n",
        "    )\n",
        "\n",
        "    # Store attention weights tensor in model for later access\n",
        "    model.attention_layer = attention\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Walk-forward cross-validation\n",
        "# -----------------------------\n",
        "def walk_forward_cv(X, y, build_model_fn, n_splits=3, epochs=15, batch_size=32, verbose=0):\n",
        "    \"\"\"\n",
        "    Perform walk-forward validation:\n",
        "    - Split chronological data into n_splits folds.\n",
        "    - For each split, train on all data up to that point and test on the next chunk.\n",
        "    \"\"\"\n",
        "    n_samples = len(X)\n",
        "    fold_size = n_samples // (n_splits + 1)  # +1 so we always have future window\n",
        "\n",
        "    rmses = []\n",
        "\n",
        "    for i in range(n_splits):\n",
        "        train_end = fold_size * (i + 1)\n",
        "        test_end = fold_size * (i + 2)\n",
        "\n",
        "        X_train, y_train = X[:train_end], y[:train_end]\n",
        "        X_test, y_test = X[train_end:test_end], y[train_end:test_end]\n",
        "\n",
        "        model = build_model_fn()\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.1,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "        rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        rmses.append(rmse)\n",
        "\n",
        "        print(f\"Fold {i+1}/{n_splits} RMSE: {rmse:.4f}\")\n",
        "\n",
        "    avg_rmse = np.mean(rmses)\n",
        "    print(\"Average walk-forward RMSE:\", avg_rmse)\n",
        "    return avg_rmse\n",
        "\n",
        "# -----------------------------\n",
        "# 8. Hyperparameter search\n",
        "# -----------------------------\n",
        "# Define hyperparameter grid\n",
        "SEQ_LENGTH_OPTIONS = [20, 30]\n",
        "LSTM_UNITS_OPTIONS = [32, 64]\n",
        "ATTN_UNITS_OPTIONS = [16, 32]\n",
        "\n",
        "best_config = None\n",
        "best_score = float(\"inf\")\n",
        "\n",
        "for seq_len in SEQ_LENGTH_OPTIONS:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Trying sequence length = {seq_len}\")\n",
        "    # Recreate sequences for this window length using TRAIN + VAL only\n",
        "    combined_train_val = np.concatenate([train_values, val_values], axis=0)\n",
        "    X_all, y_all = create_sequences(combined_train_val, target_index, seq_len)\n",
        "\n",
        "    for lstm_units in LSTM_UNITS_OPTIONS:\n",
        "        for attn_units in ATTN_UNITS_OPTIONS:\n",
        "            print(f\"Config: seq_len={seq_len}, lstm_units={lstm_units}, attn_units={attn_units}\")\n",
        "\n",
        "            def build_model_wrapper():\n",
        "                return build_attention_lstm(\n",
        "                    input_shape=(seq_len, combined_train_val.shape[1]),\n",
        "                    lstm_units=lstm_units,\n",
        "                    attention_units=attn_units,\n",
        "                    learning_rate=1e-3\n",
        "                )\n",
        "\n",
        "            avg_rmse = walk_forward_cv(\n",
        "                X_all, y_all,\n",
        "                build_model_fn=build_model_wrapper,\n",
        "                n_splits=3,\n",
        "                epochs=12,\n",
        "                batch_size=32,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            if avg_rmse < best_score:\n",
        "                best_score = avg_rmse\n",
        "                best_config = {\n",
        "                    \"seq_len\": seq_len,\n",
        "                    \"lstm_units\": lstm_units,\n",
        "                    \"attention_units\": attn_units\n",
        "                }\n",
        "\n",
        "print(\"\\nBest hyperparameters:\", best_config)\n",
        "print(\"Best walk-forward RMSE:\", best_score)\n",
        "\n",
        "# -----------------------------\n",
        "# 9. Final Training with best Attention-LSTM\n",
        "# -----------------------------\n",
        "SEQ_LEN = best_config[\"seq_len\"]\n",
        "LSTM_UNITS = best_config[\"lstm_units\"]\n",
        "ATTN_UNITS = best_config[\"attention_units\"]\n",
        "\n",
        "# Create sequences separately for train, val, test using best seq_len\n",
        "def make_seq_for_split(values, seq_len, target_index):\n",
        "    X, y = create_sequences(values, target_index, seq_len)\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = make_seq_for_split(train_values, SEQ_LEN, target_index)\n",
        "X_val, y_val = make_seq_for_split(val_values, SEQ_LEN, target_index)\n",
        "X_test, y_test = make_seq_for_split(test_values, SEQ_LEN, target_index)\n",
        "\n",
        "print(\"Final X shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
        "\n",
        "attention_model = build_attention_lstm(\n",
        "    input_shape=(SEQ_LEN, train_values.shape[1]),\n",
        "    lstm_units=LSTM_UNITS,\n",
        "    attention_units=ATTN_UNITS,\n",
        "    learning_rate=1e-3\n",
        ")\n",
        "\n",
        "history_attn = attention_model.fit(\n",
        "    np.concatenate([X_train, X_val], axis=0),\n",
        "    np.concatenate([y_train, y_val], axis=0),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 10. Baseline LSTM (no attention)\n",
        "# -----------------------------\n",
        "baseline_model = build_baseline_lstm(\n",
        "    input_shape=(SEQ_LEN, train_values.shape[1]),\n",
        "    lstm_units=LSTM_UNITS,\n",
        "    learning_rate=1e-3\n",
        ")\n",
        "\n",
        "history_base = baseline_model.fit(\n",
        "    np.concatenate([X_train, X_val], axis=0),\n",
        "    np.concatenate([y_train, y_val], axis=0),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 11. Evaluation metrics on test set\n",
        "# -----------------------------\n",
        "def mape(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # Avoid division by zero\n",
        "    non_zero = np.where(y_true == 0, 1e-8, y_true)\n",
        "    return np.mean(np.abs((y_true - y_pred) / non_zero)) * 100.0\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, name=\"Model\"):\n",
        "    y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "    rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mape_val = mape(y_test, y_pred)\n",
        "    print(f\"\\n{name} Test Metrics:\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE : {mae:.4f}\")\n",
        "    print(f\"MAPE: {mape_val:.2f}%\")\n",
        "    return y_pred, {\"rmse\": rmse, \"mae\": mae, \"mape\": mape_val}\n",
        "\n",
        "attn_preds, attn_metrics = evaluate_model(attention_model, X_test, y_test, \"Attention-LSTM\")\n",
        "base_preds, base_metrics = evaluate_model(baseline_model, X_test, y_test, \"Baseline LSTM\")\n",
        "\n",
        "# -----------------------------\n",
        "# 12. Plot predictions vs actual for visual comparison\n",
        "# -----------------------------\n",
        "def plot_predictions(y_true, y_pred_attn, y_pred_base, n_points=100):\n",
        "    plt.figure()\n",
        "    idx = np.arange(n_points)\n",
        "    plt.plot(idx, y_true[:n_points], label=\"True\")\n",
        "    plt.plot(idx, y_pred_attn[:n_points], label=\"Attention-LSTM\")\n",
        "    plt.plot(idx, y_pred_base[:n_points], label=\"Baseline LSTM\")\n",
        "    plt.xlabel(\"Time step (test subset)\")\n",
        "    plt.ylabel(\"Target value\")\n",
        "    plt.title(\"Forecast comparison (first {} test points)\".format(n_points))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_predictions(y_test, attn_preds, base_preds, n_points=120)\n",
        "\n",
        "# -----------------------------\n",
        "# 13. Attention weights visualization\n",
        "# -----------------------------\n",
        "def get_attention_weights(model, X_sample):\n",
        "    \"\"\"\n",
        "    X_sample: shape (1, seq_len, num_features)\n",
        "    Returns: attention_weights (seq_len,)\n",
        "    \"\"\"\n",
        "    # Run the LSTM to get hidden states and last hidden state\n",
        "    lstm_layer = None\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, layers.LSTM):\n",
        "            lstm_layer = layer\n",
        "            break\n",
        "    if lstm_layer is None:\n",
        "        raise ValueError(\"No LSTM layer found in model.\")\n",
        "\n",
        "    # Build a submodel to get intermediate outputs\n",
        "    intermediate_model = Model(\n",
        "        inputs=model.input,\n",
        "        outputs=[lstm_layer.output]  # (seq_out, state_h, state_c)\n",
        "    )\n",
        "\n",
        "    seq_outputs, state_h, state_c = intermediate_model.predict(X_sample, verbose=0)\n",
        "    attention_layer = model.attention_layer\n",
        "    context_vector, attention_weights = attention_layer(seq_outputs, state_h)\n",
        "    # attention_weights: (batch, time, 1)\n",
        "    return attention_weights.numpy().squeeze()\n",
        "\n",
        "# Choose a few random sequences from test set\n",
        "num_examples = 3\n",
        "indices = np.random.choice(len(X_test), size=num_examples, replace=False)\n",
        "\n",
        "for i, idx in enumerate(indices, 1):\n",
        "    X_sample = X_test[idx:idx+1]\n",
        "    y_true = y_test[idx]\n",
        "    y_pred = attention_model.predict(X_sample, verbose=0).flatten()[0]\n",
        "    attn_w = get_attention_weights(attention_model, X_sample)\n",
        "\n",
        "    print(f\"\\nExample {i}\")\n",
        "    print(f\"True target: {y_true:.4f}, Predicted: {y_pred:.4f}\")\n",
        "    print(\"Attention weights (first 10 timesteps):\", attn_w[:10])\n",
        "\n",
        "    # Plot attention distribution over time steps\n",
        "    plt.figure()\n",
        "    plt.stem(range(1, len(attn_w)+1), attn_w, use_line_collection=True)\n",
        "    plt.xlabel(\"Time step within input window\")\n",
        "    plt.ylabel(\"Attention weight\")\n",
        "    plt.title(f\"Attention distribution for example {i}\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- SUMMARY TEXT HINTS FOR YOUR REPORT ---\")\n",
        "print(\"1) Mention that you used synthetic multivariate financial-like time series generated with make_regression,\")\n",
        "print(\"   added non-linear trend and noise, and performed scaling + windowing.\")\n",
        "print(\"2) Explain that hyperparameter tuning was done via manual grid search and walk-forward CV.\")\n",
        "print(\"3) Compare RMSE/MAE/MAPE between Attention-LSTM and baseline LSTM using the printed metrics.\")\n",
        "print(\"4) Use the attention plots to describe which past time steps the model considers most important.\")\n"
      ]
    }
  ]
}